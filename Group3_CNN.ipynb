{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset_phishing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>length_url</th>\n",
       "      <th>length_hostname</th>\n",
       "      <th>ip</th>\n",
       "      <th>nb_dots</th>\n",
       "      <th>nb_hyphens</th>\n",
       "      <th>nb_at</th>\n",
       "      <th>nb_qm</th>\n",
       "      <th>nb_and</th>\n",
       "      <th>nb_or</th>\n",
       "      <th>...</th>\n",
       "      <th>domain_in_title</th>\n",
       "      <th>domain_with_copyright</th>\n",
       "      <th>whois_registered_domain</th>\n",
       "      <th>domain_registration_length</th>\n",
       "      <th>domain_age</th>\n",
       "      <th>web_traffic</th>\n",
       "      <th>dns_record</th>\n",
       "      <th>google_index</th>\n",
       "      <th>page_rank</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.crestonwood.com/router.php</td>\n",
       "      <td>37</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://shadetreetechnology.com/V4/validation/a...</td>\n",
       "      <td>77</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>5767</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://support-appleld.com.secureupdate.duila...</td>\n",
       "      <td>126</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>4004</td>\n",
       "      <td>5828815</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://rgipt.ac.in</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>-1</td>\n",
       "      <td>107721</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.iracing.com/tracks/gateway-motorspo...</td>\n",
       "      <td>55</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>8175</td>\n",
       "      <td>8725</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11425</th>\n",
       "      <td>http://www.fontspace.com/category/blackletter</td>\n",
       "      <td>45</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>448</td>\n",
       "      <td>5396</td>\n",
       "      <td>3980</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11426</th>\n",
       "      <td>http://www.budgetbots.com/server.php/Server%20...</td>\n",
       "      <td>84</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>6728</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11427</th>\n",
       "      <td>https://www.facebook.com/Interactive-Televisio...</td>\n",
       "      <td>105</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2809</td>\n",
       "      <td>8515</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11428</th>\n",
       "      <td>http://www.mypublicdomainpictures.com/</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>2836</td>\n",
       "      <td>2455493</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11429</th>\n",
       "      <td>http://174.139.46.123/ap/signin?openid.pape.ma...</td>\n",
       "      <td>477</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11430 rows Ã— 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  length_url  \\\n",
       "0                  http://www.crestonwood.com/router.php          37   \n",
       "1      http://shadetreetechnology.com/V4/validation/a...          77   \n",
       "2      https://support-appleld.com.secureupdate.duila...         126   \n",
       "3                                     http://rgipt.ac.in          18   \n",
       "4      http://www.iracing.com/tracks/gateway-motorspo...          55   \n",
       "...                                                  ...         ...   \n",
       "11425      http://www.fontspace.com/category/blackletter          45   \n",
       "11426  http://www.budgetbots.com/server.php/Server%20...          84   \n",
       "11427  https://www.facebook.com/Interactive-Televisio...         105   \n",
       "11428             http://www.mypublicdomainpictures.com/          38   \n",
       "11429  http://174.139.46.123/ap/signin?openid.pape.ma...         477   \n",
       "\n",
       "       length_hostname  ip  nb_dots  nb_hyphens  nb_at  nb_qm  nb_and  nb_or  \\\n",
       "0                   19   0        3           0      0      0       0      0   \n",
       "1                   23   1        1           0      0      0       0      0   \n",
       "2                   50   1        4           1      0      1       2      0   \n",
       "3                   11   0        2           0      0      0       0      0   \n",
       "4                   15   0        2           2      0      0       0      0   \n",
       "...                ...  ..      ...         ...    ...    ...     ...    ...   \n",
       "11425               17   0        2           0      0      0       0      0   \n",
       "11426               18   0        5           0      1      1       0      0   \n",
       "11427               16   1        2           6      0      1       0      0   \n",
       "11428               30   0        2           0      0      0       0      0   \n",
       "11429               14   1       24           0      1      1       9      0   \n",
       "\n",
       "       ...  domain_in_title  domain_with_copyright  whois_registered_domain  \\\n",
       "0      ...                0                      1                        0   \n",
       "1      ...                1                      0                        0   \n",
       "2      ...                1                      0                        0   \n",
       "3      ...                1                      0                        0   \n",
       "4      ...                0                      1                        0   \n",
       "...    ...              ...                    ...                      ...   \n",
       "11425  ...                0                      0                        0   \n",
       "11426  ...                1                      0                        0   \n",
       "11427  ...                0                      0                        0   \n",
       "11428  ...                1                      0                        0   \n",
       "11429  ...                1                      1                        1   \n",
       "\n",
       "       domain_registration_length  domain_age  web_traffic  dns_record  \\\n",
       "0                              45          -1            0           1   \n",
       "1                              77        5767            0           0   \n",
       "2                              14        4004      5828815           0   \n",
       "3                              62          -1       107721           0   \n",
       "4                             224        8175         8725           0   \n",
       "...                           ...         ...          ...         ...   \n",
       "11425                         448        5396         3980           0   \n",
       "11426                         211        6728            0           0   \n",
       "11427                        2809        8515            8           0   \n",
       "11428                          85        2836      2455493           0   \n",
       "11429                           0          -1            0           1   \n",
       "\n",
       "       google_index  page_rank      status  \n",
       "0                 1          4  legitimate  \n",
       "1                 1          2    phishing  \n",
       "2                 1          0    phishing  \n",
       "3                 0          3  legitimate  \n",
       "4                 0          6  legitimate  \n",
       "...             ...        ...         ...  \n",
       "11425             0          6  legitimate  \n",
       "11426             1          0    phishing  \n",
       "11427             1         10  legitimate  \n",
       "11428             0          4  legitimate  \n",
       "11429             1          0    phishing  \n",
       "\n",
       "[11430 rows x 89 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nb_or',\n",
       " 'ratio_nullHyperlinks',\n",
       " 'ratio_intRedirection',\n",
       " 'ratio_intErrors',\n",
       " 'submit_email',\n",
       " 'sfh']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identical_columns = []\n",
    "for column in dataset.describe().columns:\n",
    "    if dataset.describe().loc['std',column] == 0 :\n",
    "        identical_columns.append(column)\n",
    "identical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in identical_columns:\n",
    "    del dataset[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['target'] = pd.get_dummies(dataset['status'])['legitimate'].astype('int')\n",
    "del dataset['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11430, 81), (11430,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sepratating & assigning features and target columns to X & y\n",
    "y = dataset['target']\n",
    "X = dataset.drop('url',axis=1)\n",
    "X = X.drop('target',axis=1)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9144, 81), (2286, 81))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into train and test sets: 80-20 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 3)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape is  [(81, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Convert the values to 0 and 1\n",
    "from sklearn import preprocessing\n",
    "X_train = preprocessing.MinMaxScaler().fit_transform(X_train.values)\n",
    "X_test =  preprocessing.MinMaxScaler().fit_transform(X_test.values)\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "input_shape = [X_train[1].shape]\n",
    "print(\"Input shape is \", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_351\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1396 (Ba (None, 81, 1)             4         \n",
      "_________________________________________________________________\n",
      "conv1d_356 (Conv1D)          (None, 81, 16)            32        \n",
      "_________________________________________________________________\n",
      "dropout_706 (Dropout)        (None, 81, 16)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1397 (Ba (None, 81, 16)            64        \n",
      "_________________________________________________________________\n",
      "flatten_351 (Flatten)        (None, 1296)              0         \n",
      "_________________________________________________________________\n",
      "dense_1041 (Dense)           (None, 128)               166016    \n",
      "_________________________________________________________________\n",
      "dropout_707 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1398 (Ba (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1042 (Dense)           (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 166,757\n",
      "Trainable params: 166,467\n",
      "Non-trainable params: 290\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "filter_size = 16 # \n",
    "kernel_size = 1\n",
    "activation = 'tanh'\n",
    "drop_out = 0.3\n",
    "units = 128\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "model = keras.Sequential([\n",
    "    layers.BatchNormalization(input_shape=(81, 1)),\n",
    "#     layers.Conv1D(filters=4*filter_size,kernel_size=kernel_size),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(drop_out),\n",
    "    \n",
    "#     layers.Conv1D(filters=2*filter_size,kernel_size=kernel_size),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(drop_out),\n",
    "\n",
    "#     layers.Conv1D(filters=2*filter_size,kernel_size=kernel_size),\n",
    "#     layers.Dropout(drop_out),\n",
    "#     layers.BatchNormalization(),\n",
    "    \n",
    "    layers.Conv1D(filters=filter_size,kernel_size=kernel_size),\n",
    "    layers.Dropout(drop_out),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "\n",
    "    layers.Flatten(), # flatten out the layers\n",
    "    layers.Dense(units=units,activation=activation),\n",
    "    layers.Dropout(drop_out),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "#     layers.Dense(64,activation=activation),\n",
    "#     layers.BatchNormalization(),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy',keras.metrics.Precision(),keras.metrics.Recall()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "72/72 [==============================] - 2s 14ms/step - loss: 0.3122 - binary_accuracy: 0.8684 - precision_5: 0.8563 - recall_5: 0.8821 - val_loss: 0.3645 - val_binary_accuracy: 0.8718 - val_precision_5: 0.9792 - val_recall_5: 0.7656\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.1858 - binary_accuracy: 0.9315 - precision_5: 0.9224 - recall_5: 0.9392 - val_loss: 0.2696 - val_binary_accuracy: 0.9016 - val_precision_5: 0.9768 - val_recall_5: 0.8272\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1761 - binary_accuracy: 0.9347 - precision_5: 0.9245 - recall_5: 0.9448 - val_loss: 0.1696 - val_binary_accuracy: 0.9374 - val_precision_5: 0.9540 - val_recall_5: 0.9222\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.1765 - binary_accuracy: 0.9340 - precision_5: 0.9263 - recall_5: 0.9401 - val_loss: 0.1668 - val_binary_accuracy: 0.9374 - val_precision_5: 0.9638 - val_recall_5: 0.9119\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1631 - binary_accuracy: 0.9401 - precision_5: 0.9386 - recall_5: 0.9411 - val_loss: 0.1473 - val_binary_accuracy: 0.9440 - val_precision_5: 0.9506 - val_recall_5: 0.9393\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1556 - binary_accuracy: 0.9419 - precision_5: 0.9365 - recall_5: 0.9482 - val_loss: 0.1379 - val_binary_accuracy: 0.9523 - val_precision_5: 0.9476 - val_recall_5: 0.9598\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1462 - binary_accuracy: 0.9440 - precision_5: 0.9392 - recall_5: 0.9488 - val_loss: 0.1342 - val_binary_accuracy: 0.9514 - val_precision_5: 0.9545 - val_recall_5: 0.9504\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1405 - binary_accuracy: 0.9472 - precision_5: 0.9423 - recall_5: 0.9519 - val_loss: 0.1423 - val_binary_accuracy: 0.9484 - val_precision_5: 0.9638 - val_recall_5: 0.9341\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1406 - binary_accuracy: 0.9496 - precision_5: 0.9431 - recall_5: 0.9547 - val_loss: 0.1285 - val_binary_accuracy: 0.9532 - val_precision_5: 0.9562 - val_recall_5: 0.9521\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1315 - binary_accuracy: 0.9536 - precision_5: 0.9478 - recall_5: 0.9586 - val_loss: 0.1313 - val_binary_accuracy: 0.9528 - val_precision_5: 0.9417 - val_recall_5: 0.9675\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1290 - binary_accuracy: 0.9521 - precision_5: 0.9489 - recall_5: 0.9561 - val_loss: 0.1282 - val_binary_accuracy: 0.9510 - val_precision_5: 0.9536 - val_recall_5: 0.9504\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1283 - binary_accuracy: 0.9529 - precision_5: 0.9505 - recall_5: 0.9563 - val_loss: 0.1241 - val_binary_accuracy: 0.9563 - val_precision_5: 0.9518 - val_recall_5: 0.9632\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1137 - binary_accuracy: 0.9573 - precision_5: 0.9567 - recall_5: 0.9582 - val_loss: 0.1280 - val_binary_accuracy: 0.9519 - val_precision_5: 0.9641 - val_recall_5: 0.9410\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1119 - binary_accuracy: 0.9588 - precision_5: 0.9589 - recall_5: 0.9597 - val_loss: 0.1215 - val_binary_accuracy: 0.9549 - val_precision_5: 0.9587 - val_recall_5: 0.9530\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1196 - binary_accuracy: 0.9558 - precision_5: 0.9524 - recall_5: 0.9589 - val_loss: 0.1239 - val_binary_accuracy: 0.9545 - val_precision_5: 0.9571 - val_recall_5: 0.9538\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1202 - binary_accuracy: 0.9531 - precision_5: 0.9533 - recall_5: 0.9540 - val_loss: 0.1204 - val_binary_accuracy: 0.9541 - val_precision_5: 0.9524 - val_recall_5: 0.9581\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1153 - binary_accuracy: 0.9569 - precision_5: 0.9597 - recall_5: 0.9545 - val_loss: 0.1230 - val_binary_accuracy: 0.9541 - val_precision_5: 0.9570 - val_recall_5: 0.9530\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1191 - binary_accuracy: 0.9553 - precision_5: 0.9509 - recall_5: 0.9595 - val_loss: 0.1253 - val_binary_accuracy: 0.9532 - val_precision_5: 0.9455 - val_recall_5: 0.9641\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1156 - binary_accuracy: 0.9573 - precision_5: 0.9544 - recall_5: 0.9605 - val_loss: 0.1199 - val_binary_accuracy: 0.9554 - val_precision_5: 0.9564 - val_recall_5: 0.9564\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0984 - binary_accuracy: 0.9649 - precision_5: 0.9573 - recall_5: 0.9725 - val_loss: 0.1181 - val_binary_accuracy: 0.9589 - val_precision_5: 0.9622 - val_recall_5: 0.9572\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0958 - binary_accuracy: 0.9650 - precision_5: 0.9610 - recall_5: 0.9687 - val_loss: 0.1209 - val_binary_accuracy: 0.9563 - val_precision_5: 0.9580 - val_recall_5: 0.9564\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 1s 7ms/step - loss: 0.1029 - binary_accuracy: 0.9625 - precision_5: 0.9601 - recall_5: 0.9654 - val_loss: 0.1174 - val_binary_accuracy: 0.9602 - val_precision_5: 0.9607 - val_recall_5: 0.9615\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1048 - binary_accuracy: 0.9633 - precision_5: 0.9594 - recall_5: 0.9658 - val_loss: 0.1186 - val_binary_accuracy: 0.9558 - val_precision_5: 0.9627 - val_recall_5: 0.9504\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.0960 - binary_accuracy: 0.9648 - precision_5: 0.9625 - recall_5: 0.9672 - val_loss: 0.1176 - val_binary_accuracy: 0.9567 - val_precision_5: 0.9580 - val_recall_5: 0.9572\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.1040 - binary_accuracy: 0.9616 - precision_5: 0.9585 - recall_5: 0.9638 - val_loss: 0.1226 - val_binary_accuracy: 0.9576 - val_precision_5: 0.9637 - val_recall_5: 0.9530\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0999 - binary_accuracy: 0.9640 - precision_5: 0.9606 - recall_5: 0.9667 - val_loss: 0.1218 - val_binary_accuracy: 0.9589 - val_precision_5: 0.9670 - val_recall_5: 0.9521\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0949 - binary_accuracy: 0.9658 - precision_5: 0.9652 - recall_5: 0.9660 - val_loss: 0.1149 - val_binary_accuracy: 0.9589 - val_precision_5: 0.9614 - val_recall_5: 0.9581\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.0940 - binary_accuracy: 0.9654 - precision_5: 0.9592 - recall_5: 0.9709 - val_loss: 0.1192 - val_binary_accuracy: 0.9589 - val_precision_5: 0.9646 - val_recall_5: 0.9547\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0959 - binary_accuracy: 0.9645 - precision_5: 0.9628 - recall_5: 0.9663 - val_loss: 0.1196 - val_binary_accuracy: 0.9567 - val_precision_5: 0.9668 - val_recall_5: 0.9478\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0981 - binary_accuracy: 0.9629 - precision_5: 0.9628 - recall_5: 0.9636 - val_loss: 0.1136 - val_binary_accuracy: 0.9584 - val_precision_5: 0.9559 - val_recall_5: 0.9632\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0980 - binary_accuracy: 0.9654 - precision_5: 0.9606 - recall_5: 0.9694 - val_loss: 0.1199 - val_binary_accuracy: 0.9593 - val_precision_5: 0.9638 - val_recall_5: 0.9564\n",
      "Epoch 32/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0924 - binary_accuracy: 0.9647 - precision_5: 0.9648 - recall_5: 0.9644 - val_loss: 0.1160 - val_binary_accuracy: 0.9598 - val_precision_5: 0.9614 - val_recall_5: 0.9598\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0901 - binary_accuracy: 0.9661 - precision_5: 0.9621 - recall_5: 0.9699 - val_loss: 0.1161 - val_binary_accuracy: 0.9589 - val_precision_5: 0.9630 - val_recall_5: 0.9564\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0967 - binary_accuracy: 0.9628 - precision_5: 0.9592 - recall_5: 0.9663 - val_loss: 0.1163 - val_binary_accuracy: 0.9619 - val_precision_5: 0.9608 - val_recall_5: 0.9649\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0837 - binary_accuracy: 0.9682 - precision_5: 0.9649 - recall_5: 0.9715 - val_loss: 0.1129 - val_binary_accuracy: 0.9606 - val_precision_5: 0.9639 - val_recall_5: 0.9589\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0998 - binary_accuracy: 0.9631 - precision_5: 0.9655 - recall_5: 0.9626 - val_loss: 0.1204 - val_binary_accuracy: 0.9593 - val_precision_5: 0.9646 - val_recall_5: 0.9555\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0858 - binary_accuracy: 0.9726 - precision_5: 0.9742 - recall_5: 0.9710 - val_loss: 0.1133 - val_binary_accuracy: 0.9611 - val_precision_5: 0.9631 - val_recall_5: 0.9607\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0843 - binary_accuracy: 0.9694 - precision_5: 0.9661 - recall_5: 0.9721 - val_loss: 0.1143 - val_binary_accuracy: 0.9593 - val_precision_5: 0.9630 - val_recall_5: 0.9572\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0794 - binary_accuracy: 0.9686 - precision_5: 0.9680 - recall_5: 0.9688 - val_loss: 0.1251 - val_binary_accuracy: 0.9545 - val_precision_5: 0.9708 - val_recall_5: 0.9393\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0830 - binary_accuracy: 0.9692 - precision_5: 0.9667 - recall_5: 0.9705 - val_loss: 0.1120 - val_binary_accuracy: 0.9641 - val_precision_5: 0.9602 - val_recall_5: 0.9701\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0818 - binary_accuracy: 0.9706 - precision_5: 0.9673 - recall_5: 0.9732 - val_loss: 0.1179 - val_binary_accuracy: 0.9637 - val_precision_5: 0.9697 - val_recall_5: 0.9589\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0891 - binary_accuracy: 0.9662 - precision_5: 0.9657 - recall_5: 0.9674 - val_loss: 0.1126 - val_binary_accuracy: 0.9589 - val_precision_5: 0.9678 - val_recall_5: 0.9512\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0810 - binary_accuracy: 0.9702 - precision_5: 0.9671 - recall_5: 0.9735 - val_loss: 0.1162 - val_binary_accuracy: 0.9615 - val_precision_5: 0.9632 - val_recall_5: 0.9615\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0817 - binary_accuracy: 0.9686 - precision_5: 0.9643 - recall_5: 0.9726 - val_loss: 0.1224 - val_binary_accuracy: 0.9571 - val_precision_5: 0.9661 - val_recall_5: 0.9495\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0784 - binary_accuracy: 0.9706 - precision_5: 0.9693 - recall_5: 0.9707 - val_loss: 0.1134 - val_binary_accuracy: 0.9624 - val_precision_5: 0.9672 - val_recall_5: 0.9589\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0783 - binary_accuracy: 0.9720 - precision_5: 0.9714 - recall_5: 0.9727 - val_loss: 0.1141 - val_binary_accuracy: 0.9598 - val_precision_5: 0.9575 - val_recall_5: 0.9641\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0819 - binary_accuracy: 0.9686 - precision_5: 0.9673 - recall_5: 0.9693 - val_loss: 0.1185 - val_binary_accuracy: 0.9606 - val_precision_5: 0.9687 - val_recall_5: 0.9538\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.0796 - binary_accuracy: 0.9727 - precision_5: 0.9704 - recall_5: 0.9746 - val_loss: 0.1171 - val_binary_accuracy: 0.9589 - val_precision_5: 0.9638 - val_recall_5: 0.9555\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0801 - binary_accuracy: 0.9694 - precision_5: 0.9703 - recall_5: 0.9686 - val_loss: 0.1108 - val_binary_accuracy: 0.9611 - val_precision_5: 0.9663 - val_recall_5: 0.9572\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.0793 - binary_accuracy: 0.9711 - precision_5: 0.9719 - recall_5: 0.9706 - val_loss: 0.1139 - val_binary_accuracy: 0.9611 - val_precision_5: 0.9671 - val_recall_5: 0.9564\n"
     ]
    }
   ],
   "source": [
    "# stop = keras.callbacks.EarlyStopping(\n",
    "#     patience=20,\n",
    "#     min_delta=0.0001,\n",
    "#     restore_best_weights=True,\n",
    "# )\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "#     callbacks=[stop],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>binary_accuracy</th>\n",
       "      <th>precision_5</th>\n",
       "      <th>recall_5</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_binary_accuracy</th>\n",
       "      <th>val_precision_5</th>\n",
       "      <th>val_recall_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.246070</td>\n",
       "      <td>0.902668</td>\n",
       "      <td>0.896529</td>\n",
       "      <td>0.909151</td>\n",
       "      <td>0.364507</td>\n",
       "      <td>0.871828</td>\n",
       "      <td>0.979212</td>\n",
       "      <td>0.765612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.194205</td>\n",
       "      <td>0.927603</td>\n",
       "      <td>0.924203</td>\n",
       "      <td>0.930708</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>0.901575</td>\n",
       "      <td>0.976768</td>\n",
       "      <td>0.827203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.175770</td>\n",
       "      <td>0.935258</td>\n",
       "      <td>0.929409</td>\n",
       "      <td>0.941267</td>\n",
       "      <td>0.169613</td>\n",
       "      <td>0.937445</td>\n",
       "      <td>0.953982</td>\n",
       "      <td>0.922156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.168812</td>\n",
       "      <td>0.937336</td>\n",
       "      <td>0.931379</td>\n",
       "      <td>0.943467</td>\n",
       "      <td>0.166770</td>\n",
       "      <td>0.937445</td>\n",
       "      <td>0.963834</td>\n",
       "      <td>0.911891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.163095</td>\n",
       "      <td>0.939414</td>\n",
       "      <td>0.938296</td>\n",
       "      <td>0.939947</td>\n",
       "      <td>0.147285</td>\n",
       "      <td>0.944007</td>\n",
       "      <td>0.950649</td>\n",
       "      <td>0.939264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.153506</td>\n",
       "      <td>0.944991</td>\n",
       "      <td>0.941665</td>\n",
       "      <td>0.948086</td>\n",
       "      <td>0.137859</td>\n",
       "      <td>0.952318</td>\n",
       "      <td>0.947635</td>\n",
       "      <td>0.959795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.144261</td>\n",
       "      <td>0.947178</td>\n",
       "      <td>0.944432</td>\n",
       "      <td>0.949626</td>\n",
       "      <td>0.134179</td>\n",
       "      <td>0.951444</td>\n",
       "      <td>0.954467</td>\n",
       "      <td>0.950385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.143596</td>\n",
       "      <td>0.945101</td>\n",
       "      <td>0.942838</td>\n",
       "      <td>0.946986</td>\n",
       "      <td>0.142331</td>\n",
       "      <td>0.948381</td>\n",
       "      <td>0.963813</td>\n",
       "      <td>0.934132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.135806</td>\n",
       "      <td>0.949803</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.952706</td>\n",
       "      <td>0.128460</td>\n",
       "      <td>0.953193</td>\n",
       "      <td>0.956186</td>\n",
       "      <td>0.952096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.130994</td>\n",
       "      <td>0.952428</td>\n",
       "      <td>0.949683</td>\n",
       "      <td>0.954905</td>\n",
       "      <td>0.131342</td>\n",
       "      <td>0.952756</td>\n",
       "      <td>0.941715</td>\n",
       "      <td>0.967494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.127291</td>\n",
       "      <td>0.951662</td>\n",
       "      <td>0.948035</td>\n",
       "      <td>0.955125</td>\n",
       "      <td>0.128154</td>\n",
       "      <td>0.951006</td>\n",
       "      <td>0.953648</td>\n",
       "      <td>0.950385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.122334</td>\n",
       "      <td>0.954396</td>\n",
       "      <td>0.950666</td>\n",
       "      <td>0.957985</td>\n",
       "      <td>0.124061</td>\n",
       "      <td>0.956255</td>\n",
       "      <td>0.951817</td>\n",
       "      <td>0.963216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.122513</td>\n",
       "      <td>0.954068</td>\n",
       "      <td>0.950240</td>\n",
       "      <td>0.957765</td>\n",
       "      <td>0.128045</td>\n",
       "      <td>0.951881</td>\n",
       "      <td>0.964067</td>\n",
       "      <td>0.940975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.120917</td>\n",
       "      <td>0.956037</td>\n",
       "      <td>0.955185</td>\n",
       "      <td>0.956445</td>\n",
       "      <td>0.121539</td>\n",
       "      <td>0.954943</td>\n",
       "      <td>0.958692</td>\n",
       "      <td>0.952951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.121774</td>\n",
       "      <td>0.955490</td>\n",
       "      <td>0.952152</td>\n",
       "      <td>0.958645</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>0.954506</td>\n",
       "      <td>0.957082</td>\n",
       "      <td>0.953807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.118084</td>\n",
       "      <td>0.955271</td>\n",
       "      <td>0.950163</td>\n",
       "      <td>0.960405</td>\n",
       "      <td>0.120377</td>\n",
       "      <td>0.954068</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.958084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.114822</td>\n",
       "      <td>0.958224</td>\n",
       "      <td>0.957381</td>\n",
       "      <td>0.958645</td>\n",
       "      <td>0.122985</td>\n",
       "      <td>0.954068</td>\n",
       "      <td>0.957045</td>\n",
       "      <td>0.952951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.114815</td>\n",
       "      <td>0.956693</td>\n",
       "      <td>0.954446</td>\n",
       "      <td>0.958645</td>\n",
       "      <td>0.125316</td>\n",
       "      <td>0.953193</td>\n",
       "      <td>0.945470</td>\n",
       "      <td>0.964072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.112957</td>\n",
       "      <td>0.959208</td>\n",
       "      <td>0.956864</td>\n",
       "      <td>0.961285</td>\n",
       "      <td>0.119863</td>\n",
       "      <td>0.955381</td>\n",
       "      <td>0.956373</td>\n",
       "      <td>0.956373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.107586</td>\n",
       "      <td>0.960958</td>\n",
       "      <td>0.957015</td>\n",
       "      <td>0.964804</td>\n",
       "      <td>0.118133</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>0.962167</td>\n",
       "      <td>0.957228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.103915</td>\n",
       "      <td>0.961942</td>\n",
       "      <td>0.958097</td>\n",
       "      <td>0.965684</td>\n",
       "      <td>0.120917</td>\n",
       "      <td>0.956255</td>\n",
       "      <td>0.958012</td>\n",
       "      <td>0.956373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.107384</td>\n",
       "      <td>0.960849</td>\n",
       "      <td>0.958808</td>\n",
       "      <td>0.962604</td>\n",
       "      <td>0.117398</td>\n",
       "      <td>0.960193</td>\n",
       "      <td>0.960684</td>\n",
       "      <td>0.961506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.105211</td>\n",
       "      <td>0.961614</td>\n",
       "      <td>0.959676</td>\n",
       "      <td>0.963264</td>\n",
       "      <td>0.118585</td>\n",
       "      <td>0.955818</td>\n",
       "      <td>0.962738</td>\n",
       "      <td>0.950385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.100466</td>\n",
       "      <td>0.964130</td>\n",
       "      <td>0.961892</td>\n",
       "      <td>0.966124</td>\n",
       "      <td>0.117559</td>\n",
       "      <td>0.956693</td>\n",
       "      <td>0.958048</td>\n",
       "      <td>0.957228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.102595</td>\n",
       "      <td>0.962489</td>\n",
       "      <td>0.959144</td>\n",
       "      <td>0.965684</td>\n",
       "      <td>0.122585</td>\n",
       "      <td>0.957568</td>\n",
       "      <td>0.963668</td>\n",
       "      <td>0.952951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.098884</td>\n",
       "      <td>0.963692</td>\n",
       "      <td>0.958841</td>\n",
       "      <td>0.968544</td>\n",
       "      <td>0.121782</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>0.966985</td>\n",
       "      <td>0.952096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.090363</td>\n",
       "      <td>0.966098</td>\n",
       "      <td>0.965495</td>\n",
       "      <td>0.966344</td>\n",
       "      <td>0.114948</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>0.961373</td>\n",
       "      <td>0.958084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.096037</td>\n",
       "      <td>0.965004</td>\n",
       "      <td>0.962161</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>0.119152</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>0.964564</td>\n",
       "      <td>0.954662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.095646</td>\n",
       "      <td>0.964786</td>\n",
       "      <td>0.962955</td>\n",
       "      <td>0.966344</td>\n",
       "      <td>0.119605</td>\n",
       "      <td>0.956693</td>\n",
       "      <td>0.966841</td>\n",
       "      <td>0.947819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.094127</td>\n",
       "      <td>0.966098</td>\n",
       "      <td>0.965904</td>\n",
       "      <td>0.965904</td>\n",
       "      <td>0.113554</td>\n",
       "      <td>0.958443</td>\n",
       "      <td>0.955857</td>\n",
       "      <td>0.963216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.097632</td>\n",
       "      <td>0.964130</td>\n",
       "      <td>0.961084</td>\n",
       "      <td>0.967004</td>\n",
       "      <td>0.119860</td>\n",
       "      <td>0.959318</td>\n",
       "      <td>0.963793</td>\n",
       "      <td>0.956373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.095881</td>\n",
       "      <td>0.964239</td>\n",
       "      <td>0.963728</td>\n",
       "      <td>0.964364</td>\n",
       "      <td>0.116048</td>\n",
       "      <td>0.959755</td>\n",
       "      <td>0.961440</td>\n",
       "      <td>0.959795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.093576</td>\n",
       "      <td>0.964676</td>\n",
       "      <td>0.962339</td>\n",
       "      <td>0.966784</td>\n",
       "      <td>0.116070</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.956373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.093879</td>\n",
       "      <td>0.964676</td>\n",
       "      <td>0.962541</td>\n",
       "      <td>0.966564</td>\n",
       "      <td>0.116292</td>\n",
       "      <td>0.961942</td>\n",
       "      <td>0.960818</td>\n",
       "      <td>0.964927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.091012</td>\n",
       "      <td>0.965223</td>\n",
       "      <td>0.962987</td>\n",
       "      <td>0.967224</td>\n",
       "      <td>0.112866</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.963886</td>\n",
       "      <td>0.958939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.963473</td>\n",
       "      <td>0.961033</td>\n",
       "      <td>0.965684</td>\n",
       "      <td>0.120373</td>\n",
       "      <td>0.959318</td>\n",
       "      <td>0.964594</td>\n",
       "      <td>0.955518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.089531</td>\n",
       "      <td>0.969707</td>\n",
       "      <td>0.968195</td>\n",
       "      <td>0.970963</td>\n",
       "      <td>0.113339</td>\n",
       "      <td>0.961067</td>\n",
       "      <td>0.963122</td>\n",
       "      <td>0.960650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.085660</td>\n",
       "      <td>0.967738</td>\n",
       "      <td>0.967040</td>\n",
       "      <td>0.968104</td>\n",
       "      <td>0.114342</td>\n",
       "      <td>0.959318</td>\n",
       "      <td>0.962995</td>\n",
       "      <td>0.957228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.086408</td>\n",
       "      <td>0.967957</td>\n",
       "      <td>0.967055</td>\n",
       "      <td>0.968544</td>\n",
       "      <td>0.125098</td>\n",
       "      <td>0.954506</td>\n",
       "      <td>0.970822</td>\n",
       "      <td>0.939264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.090771</td>\n",
       "      <td>0.967629</td>\n",
       "      <td>0.967444</td>\n",
       "      <td>0.967444</td>\n",
       "      <td>0.112020</td>\n",
       "      <td>0.964130</td>\n",
       "      <td>0.960203</td>\n",
       "      <td>0.970060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.084836</td>\n",
       "      <td>0.968941</td>\n",
       "      <td>0.967120</td>\n",
       "      <td>0.970524</td>\n",
       "      <td>0.117935</td>\n",
       "      <td>0.963692</td>\n",
       "      <td>0.969723</td>\n",
       "      <td>0.958939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.085702</td>\n",
       "      <td>0.966317</td>\n",
       "      <td>0.965305</td>\n",
       "      <td>0.967004</td>\n",
       "      <td>0.112631</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>0.967798</td>\n",
       "      <td>0.951240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.084052</td>\n",
       "      <td>0.968176</td>\n",
       "      <td>0.967685</td>\n",
       "      <td>0.968324</td>\n",
       "      <td>0.116237</td>\n",
       "      <td>0.961505</td>\n",
       "      <td>0.963153</td>\n",
       "      <td>0.961506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.087228</td>\n",
       "      <td>0.968176</td>\n",
       "      <td>0.964216</td>\n",
       "      <td>0.972063</td>\n",
       "      <td>0.122378</td>\n",
       "      <td>0.957130</td>\n",
       "      <td>0.966057</td>\n",
       "      <td>0.949530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.084921</td>\n",
       "      <td>0.969379</td>\n",
       "      <td>0.969204</td>\n",
       "      <td>0.969204</td>\n",
       "      <td>0.113399</td>\n",
       "      <td>0.962380</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.958939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.084829</td>\n",
       "      <td>0.969051</td>\n",
       "      <td>0.966718</td>\n",
       "      <td>0.971183</td>\n",
       "      <td>0.114119</td>\n",
       "      <td>0.959755</td>\n",
       "      <td>0.957519</td>\n",
       "      <td>0.964072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.080797</td>\n",
       "      <td>0.970144</td>\n",
       "      <td>0.967608</td>\n",
       "      <td>0.972503</td>\n",
       "      <td>0.118496</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.968723</td>\n",
       "      <td>0.953807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.089381</td>\n",
       "      <td>0.967520</td>\n",
       "      <td>0.965797</td>\n",
       "      <td>0.968984</td>\n",
       "      <td>0.117127</td>\n",
       "      <td>0.958880</td>\n",
       "      <td>0.963762</td>\n",
       "      <td>0.955518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.084247</td>\n",
       "      <td>0.968504</td>\n",
       "      <td>0.966681</td>\n",
       "      <td>0.970084</td>\n",
       "      <td>0.110802</td>\n",
       "      <td>0.961067</td>\n",
       "      <td>0.966321</td>\n",
       "      <td>0.957228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.078099</td>\n",
       "      <td>0.970472</td>\n",
       "      <td>0.968449</td>\n",
       "      <td>0.972283</td>\n",
       "      <td>0.113852</td>\n",
       "      <td>0.961067</td>\n",
       "      <td>0.967128</td>\n",
       "      <td>0.956373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  binary_accuracy  precision_5  recall_5  val_loss  \\\n",
       "0   0.246070         0.902668     0.896529  0.909151  0.364507   \n",
       "1   0.194205         0.927603     0.924203  0.930708  0.269600   \n",
       "2   0.175770         0.935258     0.929409  0.941267  0.169613   \n",
       "3   0.168812         0.937336     0.931379  0.943467  0.166770   \n",
       "4   0.163095         0.939414     0.938296  0.939947  0.147285   \n",
       "5   0.153506         0.944991     0.941665  0.948086  0.137859   \n",
       "6   0.144261         0.947178     0.944432  0.949626  0.134179   \n",
       "7   0.143596         0.945101     0.942838  0.946986  0.142331   \n",
       "8   0.135806         0.949803     0.946667  0.952706  0.128460   \n",
       "9   0.130994         0.952428     0.949683  0.954905  0.131342   \n",
       "10  0.127291         0.951662     0.948035  0.955125  0.128154   \n",
       "11  0.122334         0.954396     0.950666  0.957985  0.124061   \n",
       "12  0.122513         0.954068     0.950240  0.957765  0.128045   \n",
       "13  0.120917         0.956037     0.955185  0.956445  0.121539   \n",
       "14  0.121774         0.955490     0.952152  0.958645  0.123854   \n",
       "15  0.118084         0.955271     0.950163  0.960405  0.120377   \n",
       "16  0.114822         0.958224     0.957381  0.958645  0.122985   \n",
       "17  0.114815         0.956693     0.954446  0.958645  0.125316   \n",
       "18  0.112957         0.959208     0.956864  0.961285  0.119863   \n",
       "19  0.107586         0.960958     0.957015  0.964804  0.118133   \n",
       "20  0.103915         0.961942     0.958097  0.965684  0.120917   \n",
       "21  0.107384         0.960849     0.958808  0.962604  0.117398   \n",
       "22  0.105211         0.961614     0.959676  0.963264  0.118585   \n",
       "23  0.100466         0.964130     0.961892  0.966124  0.117559   \n",
       "24  0.102595         0.962489     0.959144  0.965684  0.122585   \n",
       "25  0.098884         0.963692     0.958841  0.968544  0.121782   \n",
       "26  0.090363         0.966098     0.965495  0.966344  0.114948   \n",
       "27  0.096037         0.965004     0.962161  0.967664  0.119152   \n",
       "28  0.095646         0.964786     0.962955  0.966344  0.119605   \n",
       "29  0.094127         0.966098     0.965904  0.965904  0.113554   \n",
       "30  0.097632         0.964130     0.961084  0.967004  0.119860   \n",
       "31  0.095881         0.964239     0.963728  0.964364  0.116048   \n",
       "32  0.093576         0.964676     0.962339  0.966784  0.116070   \n",
       "33  0.093879         0.964676     0.962541  0.966564  0.116292   \n",
       "34  0.091012         0.965223     0.962987  0.967224  0.112866   \n",
       "35  0.095816         0.963473     0.961033  0.965684  0.120373   \n",
       "36  0.089531         0.969707     0.968195  0.970963  0.113339   \n",
       "37  0.085660         0.967738     0.967040  0.968104  0.114342   \n",
       "38  0.086408         0.967957     0.967055  0.968544  0.125098   \n",
       "39  0.090771         0.967629     0.967444  0.967444  0.112020   \n",
       "40  0.084836         0.968941     0.967120  0.970524  0.117935   \n",
       "41  0.085702         0.966317     0.965305  0.967004  0.112631   \n",
       "42  0.084052         0.968176     0.967685  0.968324  0.116237   \n",
       "43  0.087228         0.968176     0.964216  0.972063  0.122378   \n",
       "44  0.084921         0.969379     0.969204  0.969204  0.113399   \n",
       "45  0.084829         0.969051     0.966718  0.971183  0.114119   \n",
       "46  0.080797         0.970144     0.967608  0.972503  0.118496   \n",
       "47  0.089381         0.967520     0.965797  0.968984  0.117127   \n",
       "48  0.084247         0.968504     0.966681  0.970084  0.110802   \n",
       "49  0.078099         0.970472     0.968449  0.972283  0.113852   \n",
       "\n",
       "    val_binary_accuracy  val_precision_5  val_recall_5  \n",
       "0              0.871828         0.979212      0.765612  \n",
       "1              0.901575         0.976768      0.827203  \n",
       "2              0.937445         0.953982      0.922156  \n",
       "3              0.937445         0.963834      0.911891  \n",
       "4              0.944007         0.950649      0.939264  \n",
       "5              0.952318         0.947635      0.959795  \n",
       "6              0.951444         0.954467      0.950385  \n",
       "7              0.948381         0.963813      0.934132  \n",
       "8              0.953193         0.956186      0.952096  \n",
       "9              0.952756         0.941715      0.967494  \n",
       "10             0.951006         0.953648      0.950385  \n",
       "11             0.956255         0.951817      0.963216  \n",
       "12             0.951881         0.964067      0.940975  \n",
       "13             0.954943         0.958692      0.952951  \n",
       "14             0.954506         0.957082      0.953807  \n",
       "15             0.954068         0.952381      0.958084  \n",
       "16             0.954068         0.957045      0.952951  \n",
       "17             0.953193         0.945470      0.964072  \n",
       "18             0.955381         0.956373      0.956373  \n",
       "19             0.958880         0.962167      0.957228  \n",
       "20             0.956255         0.958012      0.956373  \n",
       "21             0.960193         0.960684      0.961506  \n",
       "22             0.955818         0.962738      0.950385  \n",
       "23             0.956693         0.958048      0.957228  \n",
       "24             0.957568         0.963668      0.952951  \n",
       "25             0.958880         0.966985      0.952096  \n",
       "26             0.958880         0.961373      0.958084  \n",
       "27             0.958880         0.964564      0.954662  \n",
       "28             0.956693         0.966841      0.947819  \n",
       "29             0.958443         0.955857      0.963216  \n",
       "30             0.959318         0.963793      0.956373  \n",
       "31             0.959755         0.961440      0.959795  \n",
       "32             0.958880         0.962963      0.956373  \n",
       "33             0.961942         0.960818      0.964927  \n",
       "34             0.960630         0.963886      0.958939  \n",
       "35             0.959318         0.964594      0.955518  \n",
       "36             0.961067         0.963122      0.960650  \n",
       "37             0.959318         0.962995      0.957228  \n",
       "38             0.954506         0.970822      0.939264  \n",
       "39             0.964130         0.960203      0.970060  \n",
       "40             0.963692         0.969723      0.958939  \n",
       "41             0.958880         0.967798      0.951240  \n",
       "42             0.961505         0.963153      0.961506  \n",
       "43             0.957130         0.966057      0.949530  \n",
       "44             0.962380         0.967213      0.958939  \n",
       "45             0.959755         0.957519      0.964072  \n",
       "46             0.960630         0.968723      0.953807  \n",
       "47             0.958880         0.963762      0.955518  \n",
       "48             0.961067         0.966321      0.957228  \n",
       "49             0.961067         0.967128      0.956373  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss: 0.1108\n",
      "Best Validation Accuracy: 0.9641\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "# Start the plot at epoch 0\n",
    "history_df.loc[0:, ['loss', 'val_loss']].plot()\n",
    "history_df.loc[0:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n",
    "\n",
    "print((\"Best Validation Loss: {:0.4f}\" +\\\n",
    "      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n",
    "      .format(history_df['val_loss'].min(), \n",
    "              history_df['val_binary_accuracy'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9746145617087987"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = (2 * history_df['val_recall_5'].max() * history_df['val_precision_5'].max())/ (history_df['val_recall_5'].max() + history_df['val_precision_5'].max())\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.970059871673584, 0.97921222448349)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df['val_recall_5'].max(),history_df['val_precision_5'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter 1 : filter size: 8  kernel size: 1  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1231\n",
      "Best Validation Accuracy: 0.9567\n",
      "Hyperparameter 2 : filter size: 8  kernel size: 1  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1330\n",
      "Best Validation Accuracy: 0.9541\n",
      "Hyperparameter 3 : filter size: 8  kernel size: 1  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1209\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 4 : filter size: 8  kernel size: 1  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1300\n",
      "Best Validation Accuracy: 0.9545\n",
      "Hyperparameter 5 : filter size: 8  kernel size: 1  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1180\n",
      "Best Validation Accuracy: 0.9602\n",
      "Hyperparameter 6 : filter size: 8  kernel size: 1  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1160\n",
      "Best Validation Accuracy: 0.9619\n",
      "Hyperparameter 7 : filter size: 8  kernel size: 1  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1220\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 8 : filter size: 8  kernel size: 1  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1172\n",
      "Best Validation Accuracy: 0.9611\n",
      "Hyperparameter 9 : filter size: 8  kernel size: 1  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1163\n",
      "Best Validation Accuracy: 0.9624\n",
      "Hyperparameter 10 : filter size: 8  kernel size: 1  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1155\n",
      "Best Validation Accuracy: 0.9646\n",
      "Hyperparameter 11 : filter size: 8  kernel size: 1  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1159\n",
      "Best Validation Accuracy: 0.9633\n",
      "Hyperparameter 12 : filter size: 8  kernel size: 1  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1185\n",
      "Best Validation Accuracy: 0.9633\n",
      "Hyperparameter 13 : filter size: 8  kernel size: 3  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1359\n",
      "Best Validation Accuracy: 0.9536\n",
      "Hyperparameter 14 : filter size: 8  kernel size: 3  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1368\n",
      "Best Validation Accuracy: 0.9519\n",
      "Hyperparameter 15 : filter size: 8  kernel size: 3  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1316\n",
      "Best Validation Accuracy: 0.9541\n",
      "Hyperparameter 16 : filter size: 8  kernel size: 3  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1314\n",
      "Best Validation Accuracy: 0.9541\n",
      "Hyperparameter 17 : filter size: 8  kernel size: 3  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1229\n",
      "Best Validation Accuracy: 0.9563\n",
      "Hyperparameter 18 : filter size: 8  kernel size: 3  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1235\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 19 : filter size: 8  kernel size: 3  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1173\n",
      "Best Validation Accuracy: 0.9624\n",
      "Hyperparameter 20 : filter size: 8  kernel size: 3  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1217\n",
      "Best Validation Accuracy: 0.9563\n",
      "Hyperparameter 21 : filter size: 8  kernel size: 3  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1195\n",
      "Best Validation Accuracy: 0.9624\n",
      "Hyperparameter 22 : filter size: 8  kernel size: 3  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1220\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 23 : filter size: 8  kernel size: 3  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1221\n",
      "Best Validation Accuracy: 0.9611\n",
      "Hyperparameter 24 : filter size: 8  kernel size: 3  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1224\n",
      "Best Validation Accuracy: 0.9611\n",
      "Hyperparameter 25 : filter size: 8  kernel size: 5  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1315\n",
      "Best Validation Accuracy: 0.9532\n",
      "Hyperparameter 26 : filter size: 8  kernel size: 5  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1399\n",
      "Best Validation Accuracy: 0.9523\n",
      "Hyperparameter 27 : filter size: 8  kernel size: 5  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1213\n",
      "Best Validation Accuracy: 0.9576\n",
      "Hyperparameter 28 : filter size: 8  kernel size: 5  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1320\n",
      "Best Validation Accuracy: 0.9571\n",
      "Hyperparameter 29 : filter size: 8  kernel size: 5  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1252\n",
      "Best Validation Accuracy: 0.9580\n",
      "Hyperparameter 30 : filter size: 8  kernel size: 5  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1242\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 31 : filter size: 8  kernel size: 5  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1137\n",
      "Best Validation Accuracy: 0.9602\n",
      "Hyperparameter 32 : filter size: 8  kernel size: 5  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1222\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 33 : filter size: 8  kernel size: 5  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1157\n",
      "Best Validation Accuracy: 0.9650\n",
      "Hyperparameter 34 : filter size: 8  kernel size: 5  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1220\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 35 : filter size: 8  kernel size: 5  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1204\n",
      "Best Validation Accuracy: 0.9619\n",
      "Hyperparameter 36 : filter size: 8  kernel size: 5  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1263\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 37 : filter size: 8  kernel size: 10  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1336\n",
      "Best Validation Accuracy: 0.9523\n",
      "Hyperparameter 38 : filter size: 8  kernel size: 10  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1448\n",
      "Best Validation Accuracy: 0.9501\n",
      "Hyperparameter 39 : filter size: 8  kernel size: 10  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1296\n",
      "Best Validation Accuracy: 0.9567\n",
      "Hyperparameter 40 : filter size: 8  kernel size: 10  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1299\n",
      "Best Validation Accuracy: 0.9558\n",
      "Hyperparameter 41 : filter size: 8  kernel size: 10  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1246\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 42 : filter size: 8  kernel size: 10  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1231\n",
      "Best Validation Accuracy: 0.9580\n",
      "Hyperparameter 43 : filter size: 8  kernel size: 10  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1231\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 44 : filter size: 8  kernel size: 10  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1222\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 45 : filter size: 8  kernel size: 10  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1253\n",
      "Best Validation Accuracy: 0.9628\n",
      "Hyperparameter 46 : filter size: 8  kernel size: 10  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1278\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 47 : filter size: 8  kernel size: 10  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1246\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 48 : filter size: 8  kernel size: 10  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1273\n",
      "Best Validation Accuracy: 0.9580\n",
      "Hyperparameter 49 : filter size: 16  kernel size: 1  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1319\n",
      "Best Validation Accuracy: 0.9545\n",
      "Hyperparameter 50 : filter size: 16  kernel size: 1  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1293\n",
      "Best Validation Accuracy: 0.9567\n",
      "Hyperparameter 51 : filter size: 16  kernel size: 1  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1239\n",
      "Best Validation Accuracy: 0.9571\n",
      "Hyperparameter 52 : filter size: 16  kernel size: 1  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1218\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 53 : filter size: 16  kernel size: 1  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1190\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 54 : filter size: 16  kernel size: 1  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1169\n",
      "Best Validation Accuracy: 0.9611\n",
      "Hyperparameter 55 : filter size: 16  kernel size: 1  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1186\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 56 : filter size: 16  kernel size: 1  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1194\n",
      "Best Validation Accuracy: 0.9633\n",
      "Hyperparameter 57 : filter size: 16  kernel size: 1  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1191\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 58 : filter size: 16  kernel size: 1  units: 128  activation: tanh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss: 0.1154\n",
      "Best Validation Accuracy: 0.9650\n",
      "Hyperparameter 59 : filter size: 16  kernel size: 1  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1239\n",
      "Best Validation Accuracy: 0.9624\n",
      "Hyperparameter 60 : filter size: 16  kernel size: 1  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1176\n",
      "Best Validation Accuracy: 0.9641\n",
      "Hyperparameter 61 : filter size: 16  kernel size: 3  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1240\n",
      "Best Validation Accuracy: 0.9606\n",
      "Hyperparameter 62 : filter size: 16  kernel size: 3  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1373\n",
      "Best Validation Accuracy: 0.9536\n",
      "Hyperparameter 63 : filter size: 16  kernel size: 3  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1251\n",
      "Best Validation Accuracy: 0.9567\n",
      "Hyperparameter 64 : filter size: 16  kernel size: 3  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1213\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 65 : filter size: 16  kernel size: 3  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1175\n",
      "Best Validation Accuracy: 0.9615\n",
      "Hyperparameter 66 : filter size: 16  kernel size: 3  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1208\n",
      "Best Validation Accuracy: 0.9580\n",
      "Hyperparameter 67 : filter size: 16  kernel size: 3  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1206\n",
      "Best Validation Accuracy: 0.9637\n",
      "Hyperparameter 68 : filter size: 16  kernel size: 3  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1231\n",
      "Best Validation Accuracy: 0.9606\n",
      "Hyperparameter 69 : filter size: 16  kernel size: 3  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1198\n",
      "Best Validation Accuracy: 0.9624\n",
      "Hyperparameter 70 : filter size: 16  kernel size: 3  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1234\n",
      "Best Validation Accuracy: 0.9580\n",
      "Hyperparameter 71 : filter size: 16  kernel size: 3  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1234\n",
      "Best Validation Accuracy: 0.9628\n",
      "Hyperparameter 72 : filter size: 16  kernel size: 3  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1241\n",
      "Best Validation Accuracy: 0.9606\n",
      "Hyperparameter 73 : filter size: 16  kernel size: 5  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1282\n",
      "Best Validation Accuracy: 0.9532\n",
      "Hyperparameter 74 : filter size: 16  kernel size: 5  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1401\n",
      "Best Validation Accuracy: 0.9514\n",
      "Hyperparameter 75 : filter size: 16  kernel size: 5  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1309\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 76 : filter size: 16  kernel size: 5  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1247\n",
      "Best Validation Accuracy: 0.9567\n",
      "Hyperparameter 77 : filter size: 16  kernel size: 5  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1225\n",
      "Best Validation Accuracy: 0.9624\n",
      "Hyperparameter 78 : filter size: 16  kernel size: 5  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1278\n",
      "Best Validation Accuracy: 0.9554\n",
      "Hyperparameter 79 : filter size: 16  kernel size: 5  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1218\n",
      "Best Validation Accuracy: 0.9619\n",
      "Hyperparameter 80 : filter size: 16  kernel size: 5  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1221\n",
      "Best Validation Accuracy: 0.9602\n",
      "Hyperparameter 81 : filter size: 16  kernel size: 5  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1237\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 82 : filter size: 16  kernel size: 5  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1222\n",
      "Best Validation Accuracy: 0.9602\n",
      "Hyperparameter 83 : filter size: 16  kernel size: 5  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1305\n",
      "Best Validation Accuracy: 0.9602\n",
      "Hyperparameter 84 : filter size: 16  kernel size: 5  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1239\n",
      "Best Validation Accuracy: 0.9624\n",
      "Hyperparameter 85 : filter size: 16  kernel size: 10  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1341\n",
      "Best Validation Accuracy: 0.9532\n",
      "Hyperparameter 86 : filter size: 16  kernel size: 10  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1387\n",
      "Best Validation Accuracy: 0.9501\n",
      "Hyperparameter 87 : filter size: 16  kernel size: 10  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1277\n",
      "Best Validation Accuracy: 0.9549\n",
      "Hyperparameter 88 : filter size: 16  kernel size: 10  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1265\n",
      "Best Validation Accuracy: 0.9571\n",
      "Hyperparameter 89 : filter size: 16  kernel size: 10  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1204\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 90 : filter size: 16  kernel size: 10  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1274\n",
      "Best Validation Accuracy: 0.9576\n",
      "Hyperparameter 91 : filter size: 16  kernel size: 10  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1214\n",
      "Best Validation Accuracy: 0.9624\n",
      "Hyperparameter 92 : filter size: 16  kernel size: 10  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1204\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 93 : filter size: 16  kernel size: 10  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1247\n",
      "Best Validation Accuracy: 0.9606\n",
      "Hyperparameter 94 : filter size: 16  kernel size: 10  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1233\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 95 : filter size: 16  kernel size: 10  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1276\n",
      "Best Validation Accuracy: 0.9615\n",
      "Hyperparameter 96 : filter size: 16  kernel size: 10  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1249\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 97 : filter size: 32  kernel size: 1  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1306\n",
      "Best Validation Accuracy: 0.9571\n",
      "Hyperparameter 98 : filter size: 32  kernel size: 1  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1309\n",
      "Best Validation Accuracy: 0.9549\n",
      "Hyperparameter 99 : filter size: 32  kernel size: 1  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1292\n",
      "Best Validation Accuracy: 0.9567\n",
      "Hyperparameter 100 : filter size: 32  kernel size: 1  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1243\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 101 : filter size: 32  kernel size: 1  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1280\n",
      "Best Validation Accuracy: 0.9606\n",
      "Hyperparameter 102 : filter size: 32  kernel size: 1  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1204\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 103 : filter size: 32  kernel size: 1  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1227\n",
      "Best Validation Accuracy: 0.9611\n",
      "Hyperparameter 104 : filter size: 32  kernel size: 1  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1224\n",
      "Best Validation Accuracy: 0.9611\n",
      "Hyperparameter 105 : filter size: 32  kernel size: 1  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1254\n",
      "Best Validation Accuracy: 0.9615\n",
      "Hyperparameter 106 : filter size: 32  kernel size: 1  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1202\n",
      "Best Validation Accuracy: 0.9602\n",
      "Hyperparameter 107 : filter size: 32  kernel size: 1  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1287\n",
      "Best Validation Accuracy: 0.9619\n",
      "Hyperparameter 108 : filter size: 32  kernel size: 1  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1264\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 109 : filter size: 32  kernel size: 3  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1340\n",
      "Best Validation Accuracy: 0.9549\n",
      "Hyperparameter 110 : filter size: 32  kernel size: 3  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1294\n",
      "Best Validation Accuracy: 0.9554\n",
      "Hyperparameter 111 : filter size: 32  kernel size: 3  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1281\n",
      "Best Validation Accuracy: 0.9563\n",
      "Hyperparameter 112 : filter size: 32  kernel size: 3  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1295\n",
      "Best Validation Accuracy: 0.9567\n",
      "Hyperparameter 113 : filter size: 32  kernel size: 3  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1211\n",
      "Best Validation Accuracy: 0.9615\n",
      "Hyperparameter 114 : filter size: 32  kernel size: 3  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1231\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 115 : filter size: 32  kernel size: 3  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1221\n",
      "Best Validation Accuracy: 0.9615\n",
      "Hyperparameter 116 : filter size: 32  kernel size: 3  units: 64  activation: tanh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss: 0.1266\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 117 : filter size: 32  kernel size: 3  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1235\n",
      "Best Validation Accuracy: 0.9619\n",
      "Hyperparameter 118 : filter size: 32  kernel size: 3  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1201\n",
      "Best Validation Accuracy: 0.9602\n",
      "Hyperparameter 119 : filter size: 32  kernel size: 3  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1278\n",
      "Best Validation Accuracy: 0.9615\n",
      "Hyperparameter 120 : filter size: 32  kernel size: 3  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1229\n",
      "Best Validation Accuracy: 0.9611\n",
      "Hyperparameter 121 : filter size: 32  kernel size: 5  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1338\n",
      "Best Validation Accuracy: 0.9536\n",
      "Hyperparameter 122 : filter size: 32  kernel size: 5  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1438\n",
      "Best Validation Accuracy: 0.9501\n",
      "Hyperparameter 123 : filter size: 32  kernel size: 5  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1235\n",
      "Best Validation Accuracy: 0.9576\n",
      "Hyperparameter 124 : filter size: 32  kernel size: 5  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1286\n",
      "Best Validation Accuracy: 0.9558\n",
      "Hyperparameter 125 : filter size: 32  kernel size: 5  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1194\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 126 : filter size: 32  kernel size: 5  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1250\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 127 : filter size: 32  kernel size: 5  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1222\n",
      "Best Validation Accuracy: 0.9619\n",
      "Hyperparameter 128 : filter size: 32  kernel size: 5  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1221\n",
      "Best Validation Accuracy: 0.9606\n",
      "Hyperparameter 129 : filter size: 32  kernel size: 5  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1228\n",
      "Best Validation Accuracy: 0.9637\n",
      "Hyperparameter 130 : filter size: 32  kernel size: 5  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1260\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 131 : filter size: 32  kernel size: 5  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1246\n",
      "Best Validation Accuracy: 0.9619\n",
      "Hyperparameter 132 : filter size: 32  kernel size: 5  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1198\n",
      "Best Validation Accuracy: 0.9606\n",
      "Hyperparameter 133 : filter size: 32  kernel size: 10  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1331\n",
      "Best Validation Accuracy: 0.9571\n",
      "Hyperparameter 134 : filter size: 32  kernel size: 10  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1384\n",
      "Best Validation Accuracy: 0.9528\n",
      "Hyperparameter 135 : filter size: 32  kernel size: 10  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1246\n",
      "Best Validation Accuracy: 0.9615\n",
      "Hyperparameter 136 : filter size: 32  kernel size: 10  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1330\n",
      "Best Validation Accuracy: 0.9536\n",
      "Hyperparameter 137 : filter size: 32  kernel size: 10  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1201\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 138 : filter size: 32  kernel size: 10  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1237\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 139 : filter size: 32  kernel size: 10  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1240\n",
      "Best Validation Accuracy: 0.9606\n",
      "Hyperparameter 140 : filter size: 32  kernel size: 10  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1270\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 141 : filter size: 32  kernel size: 10  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1249\n",
      "Best Validation Accuracy: 0.9628\n",
      "Hyperparameter 142 : filter size: 32  kernel size: 10  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1240\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 143 : filter size: 32  kernel size: 10  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1295\n",
      "Best Validation Accuracy: 0.9628\n",
      "Hyperparameter 144 : filter size: 32  kernel size: 10  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1271\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 145 : filter size: 64  kernel size: 1  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1349\n",
      "Best Validation Accuracy: 0.9532\n",
      "Hyperparameter 146 : filter size: 64  kernel size: 1  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1379\n",
      "Best Validation Accuracy: 0.9523\n",
      "Hyperparameter 147 : filter size: 64  kernel size: 1  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1306\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 148 : filter size: 64  kernel size: 1  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1290\n",
      "Best Validation Accuracy: 0.9545\n",
      "Hyperparameter 149 : filter size: 64  kernel size: 1  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1225\n",
      "Best Validation Accuracy: 0.9650\n",
      "Hyperparameter 150 : filter size: 64  kernel size: 1  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1270\n",
      "Best Validation Accuracy: 0.9602\n",
      "Hyperparameter 151 : filter size: 64  kernel size: 1  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1229\n",
      "Best Validation Accuracy: 0.9611\n",
      "Hyperparameter 152 : filter size: 64  kernel size: 1  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1289\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 153 : filter size: 64  kernel size: 1  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1250\n",
      "Best Validation Accuracy: 0.9619\n",
      "Hyperparameter 154 : filter size: 64  kernel size: 1  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1300\n",
      "Best Validation Accuracy: 0.9571\n",
      "Hyperparameter 155 : filter size: 64  kernel size: 1  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1277\n",
      "Best Validation Accuracy: 0.9606\n",
      "Hyperparameter 156 : filter size: 64  kernel size: 1  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1254\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 157 : filter size: 64  kernel size: 3  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1344\n",
      "Best Validation Accuracy: 0.9523\n",
      "Hyperparameter 158 : filter size: 64  kernel size: 3  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1422\n",
      "Best Validation Accuracy: 0.9519\n",
      "Hyperparameter 159 : filter size: 64  kernel size: 3  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1294\n",
      "Best Validation Accuracy: 0.9576\n",
      "Hyperparameter 160 : filter size: 64  kernel size: 3  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1281\n",
      "Best Validation Accuracy: 0.9558\n",
      "Hyperparameter 161 : filter size: 64  kernel size: 3  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1227\n",
      "Best Validation Accuracy: 0.9602\n",
      "Hyperparameter 162 : filter size: 64  kernel size: 3  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1282\n",
      "Best Validation Accuracy: 0.9563\n",
      "Hyperparameter 163 : filter size: 64  kernel size: 3  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1263\n",
      "Best Validation Accuracy: 0.9615\n",
      "Hyperparameter 164 : filter size: 64  kernel size: 3  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1322\n",
      "Best Validation Accuracy: 0.9563\n",
      "Hyperparameter 165 : filter size: 64  kernel size: 3  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1222\n",
      "Best Validation Accuracy: 0.9606\n",
      "Hyperparameter 166 : filter size: 64  kernel size: 3  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1256\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 167 : filter size: 64  kernel size: 3  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1280\n",
      "Best Validation Accuracy: 0.9624\n",
      "Hyperparameter 168 : filter size: 64  kernel size: 3  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1258\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 169 : filter size: 64  kernel size: 5  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1333\n",
      "Best Validation Accuracy: 0.9563\n",
      "Hyperparameter 170 : filter size: 64  kernel size: 5  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1428\n",
      "Best Validation Accuracy: 0.9493\n",
      "Hyperparameter 171 : filter size: 64  kernel size: 5  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1303\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 172 : filter size: 64  kernel size: 5  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1346\n",
      "Best Validation Accuracy: 0.9528\n",
      "Hyperparameter 173 : filter size: 64  kernel size: 5  units: 32  activation: relu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss: 0.1301\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 174 : filter size: 64  kernel size: 5  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1299\n",
      "Best Validation Accuracy: 0.9567\n",
      "Hyperparameter 175 : filter size: 64  kernel size: 5  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1298\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 176 : filter size: 64  kernel size: 5  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1286\n",
      "Best Validation Accuracy: 0.9571\n",
      "Hyperparameter 177 : filter size: 64  kernel size: 5  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1265\n",
      "Best Validation Accuracy: 0.9606\n",
      "Hyperparameter 178 : filter size: 64  kernel size: 5  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1227\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 179 : filter size: 64  kernel size: 5  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1240\n",
      "Best Validation Accuracy: 0.9619\n",
      "Hyperparameter 180 : filter size: 64  kernel size: 5  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1230\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 181 : filter size: 64  kernel size: 10  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1375\n",
      "Best Validation Accuracy: 0.9528\n",
      "Hyperparameter 182 : filter size: 64  kernel size: 10  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1465\n",
      "Best Validation Accuracy: 0.9484\n",
      "Hyperparameter 183 : filter size: 64  kernel size: 10  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1273\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 184 : filter size: 64  kernel size: 10  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1374\n",
      "Best Validation Accuracy: 0.9528\n",
      "Hyperparameter 185 : filter size: 64  kernel size: 10  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1218\n",
      "Best Validation Accuracy: 0.9611\n",
      "Hyperparameter 186 : filter size: 64  kernel size: 10  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1339\n",
      "Best Validation Accuracy: 0.9563\n",
      "Hyperparameter 187 : filter size: 64  kernel size: 10  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1207\n",
      "Best Validation Accuracy: 0.9650\n",
      "Hyperparameter 188 : filter size: 64  kernel size: 10  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1331\n",
      "Best Validation Accuracy: 0.9554\n",
      "Hyperparameter 189 : filter size: 64  kernel size: 10  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1248\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 190 : filter size: 64  kernel size: 10  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1242\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 191 : filter size: 64  kernel size: 10  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1334\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 192 : filter size: 64  kernel size: 10  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1296\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 193 : filter size: 128  kernel size: 1  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1341\n",
      "Best Validation Accuracy: 0.9536\n",
      "Hyperparameter 194 : filter size: 128  kernel size: 1  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1427\n",
      "Best Validation Accuracy: 0.9506\n",
      "Hyperparameter 195 : filter size: 128  kernel size: 1  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1330\n",
      "Best Validation Accuracy: 0.9549\n",
      "Hyperparameter 196 : filter size: 128  kernel size: 1  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1414\n",
      "Best Validation Accuracy: 0.9528\n",
      "Hyperparameter 197 : filter size: 128  kernel size: 1  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1315\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 198 : filter size: 128  kernel size: 1  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1355\n",
      "Best Validation Accuracy: 0.9532\n",
      "Hyperparameter 199 : filter size: 128  kernel size: 1  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1309\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 200 : filter size: 128  kernel size: 1  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1364\n",
      "Best Validation Accuracy: 0.9554\n",
      "Hyperparameter 201 : filter size: 128  kernel size: 1  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1317\n",
      "Best Validation Accuracy: 0.9637\n",
      "Hyperparameter 202 : filter size: 128  kernel size: 1  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1296\n",
      "Best Validation Accuracy: 0.9571\n",
      "Hyperparameter 203 : filter size: 128  kernel size: 1  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1295\n",
      "Best Validation Accuracy: 0.9602\n",
      "Hyperparameter 204 : filter size: 128  kernel size: 1  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1334\n",
      "Best Validation Accuracy: 0.9576\n",
      "Hyperparameter 205 : filter size: 128  kernel size: 3  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1426\n",
      "Best Validation Accuracy: 0.9523\n",
      "Hyperparameter 206 : filter size: 128  kernel size: 3  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1557\n",
      "Best Validation Accuracy: 0.9475\n",
      "Hyperparameter 207 : filter size: 128  kernel size: 3  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1291\n",
      "Best Validation Accuracy: 0.9563\n",
      "Hyperparameter 208 : filter size: 128  kernel size: 3  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1435\n",
      "Best Validation Accuracy: 0.9519\n",
      "Hyperparameter 209 : filter size: 128  kernel size: 3  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1262\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 210 : filter size: 128  kernel size: 3  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1361\n",
      "Best Validation Accuracy: 0.9523\n",
      "Hyperparameter 211 : filter size: 128  kernel size: 3  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1262\n",
      "Best Validation Accuracy: 0.9633\n",
      "Hyperparameter 212 : filter size: 128  kernel size: 3  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1365\n",
      "Best Validation Accuracy: 0.9536\n",
      "Hyperparameter 213 : filter size: 128  kernel size: 3  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1238\n",
      "Best Validation Accuracy: 0.9602\n",
      "Hyperparameter 214 : filter size: 128  kernel size: 3  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1312\n",
      "Best Validation Accuracy: 0.9576\n",
      "Hyperparameter 215 : filter size: 128  kernel size: 3  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1280\n",
      "Best Validation Accuracy: 0.9641\n",
      "Hyperparameter 216 : filter size: 128  kernel size: 3  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1288\n",
      "Best Validation Accuracy: 0.9554\n",
      "Hyperparameter 217 : filter size: 128  kernel size: 5  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1322\n",
      "Best Validation Accuracy: 0.9545\n",
      "Hyperparameter 218 : filter size: 128  kernel size: 5  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1541\n",
      "Best Validation Accuracy: 0.9440\n",
      "Hyperparameter 219 : filter size: 128  kernel size: 5  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1299\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 220 : filter size: 128  kernel size: 5  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1450\n",
      "Best Validation Accuracy: 0.9488\n",
      "Hyperparameter 221 : filter size: 128  kernel size: 5  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1275\n",
      "Best Validation Accuracy: 0.9606\n",
      "Hyperparameter 222 : filter size: 128  kernel size: 5  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1395\n",
      "Best Validation Accuracy: 0.9523\n",
      "Hyperparameter 223 : filter size: 128  kernel size: 5  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1339\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 224 : filter size: 128  kernel size: 5  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1369\n",
      "Best Validation Accuracy: 0.9541\n",
      "Hyperparameter 225 : filter size: 128  kernel size: 5  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1210\n",
      "Best Validation Accuracy: 0.9615\n",
      "Hyperparameter 226 : filter size: 128  kernel size: 5  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1426\n",
      "Best Validation Accuracy: 0.9545\n",
      "Hyperparameter 227 : filter size: 128  kernel size: 5  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1315\n",
      "Best Validation Accuracy: 0.9611\n",
      "Hyperparameter 228 : filter size: 128  kernel size: 5  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1353\n",
      "Best Validation Accuracy: 0.9545\n",
      "Hyperparameter 229 : filter size: 128  kernel size: 10  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1366\n",
      "Best Validation Accuracy: 0.9528\n",
      "Hyperparameter 230 : filter size: 128  kernel size: 10  units: 8  activation: tanh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss: 0.1652\n",
      "Best Validation Accuracy: 0.9418\n",
      "Hyperparameter 231 : filter size: 128  kernel size: 10  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1316\n",
      "Best Validation Accuracy: 0.9567\n",
      "Hyperparameter 232 : filter size: 128  kernel size: 10  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1507\n",
      "Best Validation Accuracy: 0.9484\n",
      "Hyperparameter 233 : filter size: 128  kernel size: 10  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1276\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 234 : filter size: 128  kernel size: 10  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1454\n",
      "Best Validation Accuracy: 0.9510\n",
      "Hyperparameter 235 : filter size: 128  kernel size: 10  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1374\n",
      "Best Validation Accuracy: 0.9580\n",
      "Hyperparameter 236 : filter size: 128  kernel size: 10  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1422\n",
      "Best Validation Accuracy: 0.9532\n",
      "Hyperparameter 237 : filter size: 128  kernel size: 10  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1299\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 238 : filter size: 128  kernel size: 10  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1359\n",
      "Best Validation Accuracy: 0.9541\n",
      "Hyperparameter 239 : filter size: 128  kernel size: 10  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1328\n",
      "Best Validation Accuracy: 0.9611\n",
      "Hyperparameter 240 : filter size: 128  kernel size: 10  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1298\n",
      "Best Validation Accuracy: 0.9541\n",
      "Hyperparameter 241 : filter size: 256  kernel size: 1  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1373\n",
      "Best Validation Accuracy: 0.9545\n",
      "Hyperparameter 242 : filter size: 256  kernel size: 1  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1610\n",
      "Best Validation Accuracy: 0.9440\n",
      "Hyperparameter 243 : filter size: 256  kernel size: 1  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1277\n",
      "Best Validation Accuracy: 0.9576\n",
      "Hyperparameter 244 : filter size: 256  kernel size: 1  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1537\n",
      "Best Validation Accuracy: 0.9488\n",
      "Hyperparameter 245 : filter size: 256  kernel size: 1  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1391\n",
      "Best Validation Accuracy: 0.9580\n",
      "Hyperparameter 246 : filter size: 256  kernel size: 1  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1506\n",
      "Best Validation Accuracy: 0.9484\n",
      "Hyperparameter 247 : filter size: 256  kernel size: 1  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1307\n",
      "Best Validation Accuracy: 0.9580\n",
      "Hyperparameter 248 : filter size: 256  kernel size: 1  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1445\n",
      "Best Validation Accuracy: 0.9497\n",
      "Hyperparameter 249 : filter size: 256  kernel size: 1  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1343\n",
      "Best Validation Accuracy: 0.9602\n",
      "Hyperparameter 250 : filter size: 256  kernel size: 1  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1384\n",
      "Best Validation Accuracy: 0.9523\n",
      "Hyperparameter 251 : filter size: 256  kernel size: 1  units: 256  activation: relu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-c81dbf706076>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m                 )\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filter_sizes = [8,16,32,64,128,256]\n",
    "kernel_sizes = [1,2,3,5,10]\n",
    "activations = ['relu','tanh']\n",
    "drop_out = 0.3\n",
    "units_list = [8,16,32,64,128,256]\n",
    "Best_acc = []\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "count = 0\n",
    "for filter_size in filter_sizes:\n",
    "    for kernel_size in kernel_sizes:\n",
    "        for units in units_list:\n",
    "            for activation in activations:\n",
    "                count+=1\n",
    "                print(\"Hyperparameter\", count, \": filter size:\", filter_size, \" kernel size:\", kernel_size, \" units:\", units, \" activation:\", activation)\n",
    "                model = keras.Sequential([\n",
    "                    layers.BatchNormalization(input_shape=(81, 1)),\n",
    "                    layers.Conv1D(filters=filter_size,kernel_size=kernel_size),\n",
    "                    layers.Dropout(drop_out),\n",
    "                    layers.BatchNormalization(),\n",
    "\n",
    "                    layers.Flatten(), # flatten out the layers\n",
    "                    layers.Dense(units=2*units,activation=activation),\n",
    "                    layers.Dropout(drop_out),\n",
    "                    layers.BatchNormalization(),\n",
    "\n",
    "                    layers.Dense(units=units,activation=activation),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.Dense(1, activation='sigmoid'),\n",
    "                ])\n",
    "\n",
    "                opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "                model.compile(\n",
    "                    optimizer=opt,\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['binary_accuracy'],\n",
    "                )\n",
    "\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=128,\n",
    "                    epochs=50,\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                history_df = pd.DataFrame(history.history)\n",
    "#                 # Start the plot at epoch 0\n",
    "#                 history_df.loc[0:, ['loss', 'val_loss']].plot(title=\"Loss Plot with hyperparameter \"+str(count))\n",
    "#                 history_df.loc[0:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Acc Plot with hyperparameter \"+str(count))\n",
    "                Best_acc.append(history_df['val_binary_accuracy'].max())\n",
    "    \n",
    "                print((\"Best Validation Loss: {:0.4f}\" +\\\n",
    "                      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n",
    "                      .format(history_df['val_loss'].min(), \n",
    "                              history_df['val_binary_accuracy'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter 1 : filter size: 256  kernel size: 1  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1316\n",
      "Best Validation Accuracy: 0.9545\n",
      "Hyperparameter 2 : filter size: 256  kernel size: 1  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1561\n",
      "Best Validation Accuracy: 0.9440\n",
      "Hyperparameter 3 : filter size: 256  kernel size: 1  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1274\n",
      "Best Validation Accuracy: 0.9541\n",
      "Hyperparameter 4 : filter size: 256  kernel size: 1  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1483\n",
      "Best Validation Accuracy: 0.9475\n",
      "Hyperparameter 5 : filter size: 256  kernel size: 1  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1372\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 6 : filter size: 256  kernel size: 1  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1454\n",
      "Best Validation Accuracy: 0.9497\n",
      "Hyperparameter 7 : filter size: 256  kernel size: 1  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1290\n",
      "Best Validation Accuracy: 0.9615\n",
      "Hyperparameter 8 : filter size: 256  kernel size: 1  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1487\n",
      "Best Validation Accuracy: 0.9514\n",
      "Hyperparameter 9 : filter size: 256  kernel size: 1  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1286\n",
      "Best Validation Accuracy: 0.9624\n",
      "Hyperparameter 10 : filter size: 256  kernel size: 1  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1491\n",
      "Best Validation Accuracy: 0.9514\n",
      "Hyperparameter 11 : filter size: 256  kernel size: 1  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1309\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 12 : filter size: 256  kernel size: 1  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1377\n",
      "Best Validation Accuracy: 0.9523\n",
      "Hyperparameter 13 : filter size: 256  kernel size: 2  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1354\n",
      "Best Validation Accuracy: 0.9554\n",
      "Hyperparameter 14 : filter size: 256  kernel size: 2  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1585\n",
      "Best Validation Accuracy: 0.9436\n",
      "Hyperparameter 15 : filter size: 256  kernel size: 2  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1325\n",
      "Best Validation Accuracy: 0.9571\n",
      "Hyperparameter 16 : filter size: 256  kernel size: 2  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1526\n",
      "Best Validation Accuracy: 0.9458\n",
      "Hyperparameter 17 : filter size: 256  kernel size: 2  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1277\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 18 : filter size: 256  kernel size: 2  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1475\n",
      "Best Validation Accuracy: 0.9488\n",
      "Hyperparameter 19 : filter size: 256  kernel size: 2  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1282\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 20 : filter size: 256  kernel size: 2  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1431\n",
      "Best Validation Accuracy: 0.9488\n",
      "Hyperparameter 21 : filter size: 256  kernel size: 2  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1300\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 22 : filter size: 256  kernel size: 2  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1425\n",
      "Best Validation Accuracy: 0.9541\n",
      "Hyperparameter 23 : filter size: 256  kernel size: 2  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1258\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 24 : filter size: 256  kernel size: 2  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1405\n",
      "Best Validation Accuracy: 0.9541\n",
      "Hyperparameter 25 : filter size: 256  kernel size: 3  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1357\n",
      "Best Validation Accuracy: 0.9545\n",
      "Hyperparameter 26 : filter size: 256  kernel size: 3  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1583\n",
      "Best Validation Accuracy: 0.9405\n",
      "Hyperparameter 27 : filter size: 256  kernel size: 3  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1372\n",
      "Best Validation Accuracy: 0.9563\n",
      "Hyperparameter 28 : filter size: 256  kernel size: 3  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1543\n",
      "Best Validation Accuracy: 0.9453\n",
      "Hyperparameter 29 : filter size: 256  kernel size: 3  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1267\n",
      "Best Validation Accuracy: 0.9611\n",
      "Hyperparameter 30 : filter size: 256  kernel size: 3  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1557\n",
      "Best Validation Accuracy: 0.9466\n",
      "Hyperparameter 31 : filter size: 256  kernel size: 3  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1236\n",
      "Best Validation Accuracy: 0.9602\n",
      "Hyperparameter 32 : filter size: 256  kernel size: 3  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1480\n",
      "Best Validation Accuracy: 0.9493\n",
      "Hyperparameter 33 : filter size: 256  kernel size: 3  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1284\n",
      "Best Validation Accuracy: 0.9633\n",
      "Hyperparameter 34 : filter size: 256  kernel size: 3  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1422\n",
      "Best Validation Accuracy: 0.9532\n",
      "Hyperparameter 35 : filter size: 256  kernel size: 3  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1267\n",
      "Best Validation Accuracy: 0.9615\n",
      "Hyperparameter 36 : filter size: 256  kernel size: 3  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1404\n",
      "Best Validation Accuracy: 0.9519\n",
      "Hyperparameter 37 : filter size: 256  kernel size: 5  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1400\n",
      "Best Validation Accuracy: 0.9532\n",
      "Hyperparameter 38 : filter size: 256  kernel size: 5  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1722\n",
      "Best Validation Accuracy: 0.9423\n",
      "Hyperparameter 39 : filter size: 256  kernel size: 5  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1211\n",
      "Best Validation Accuracy: 0.9589\n",
      "Hyperparameter 40 : filter size: 256  kernel size: 5  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1593\n",
      "Best Validation Accuracy: 0.9423\n",
      "Hyperparameter 41 : filter size: 256  kernel size: 5  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1221\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 42 : filter size: 256  kernel size: 5  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1557\n",
      "Best Validation Accuracy: 0.9466\n",
      "Hyperparameter 43 : filter size: 256  kernel size: 5  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1227\n",
      "Best Validation Accuracy: 0.9637\n",
      "Hyperparameter 44 : filter size: 256  kernel size: 5  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1503\n",
      "Best Validation Accuracy: 0.9484\n",
      "Hyperparameter 45 : filter size: 256  kernel size: 5  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1248\n",
      "Best Validation Accuracy: 0.9633\n",
      "Hyperparameter 46 : filter size: 256  kernel size: 5  units: 128  activation: tanh\n",
      "Best Validation Loss: 0.1511\n",
      "Best Validation Accuracy: 0.9514\n",
      "Hyperparameter 47 : filter size: 256  kernel size: 5  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1342\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 48 : filter size: 256  kernel size: 5  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1430\n",
      "Best Validation Accuracy: 0.9523\n",
      "Hyperparameter 49 : filter size: 256  kernel size: 10  units: 8  activation: relu\n",
      "Best Validation Loss: 0.1370\n",
      "Best Validation Accuracy: 0.9523\n",
      "Hyperparameter 50 : filter size: 256  kernel size: 10  units: 8  activation: tanh\n",
      "Best Validation Loss: 0.1732\n",
      "Best Validation Accuracy: 0.9383\n",
      "Hyperparameter 51 : filter size: 256  kernel size: 10  units: 16  activation: relu\n",
      "Best Validation Loss: 0.1284\n",
      "Best Validation Accuracy: 0.9554\n",
      "Hyperparameter 52 : filter size: 256  kernel size: 10  units: 16  activation: tanh\n",
      "Best Validation Loss: 0.1638\n",
      "Best Validation Accuracy: 0.9440\n",
      "Hyperparameter 53 : filter size: 256  kernel size: 10  units: 32  activation: relu\n",
      "Best Validation Loss: 0.1296\n",
      "Best Validation Accuracy: 0.9593\n",
      "Hyperparameter 54 : filter size: 256  kernel size: 10  units: 32  activation: tanh\n",
      "Best Validation Loss: 0.1584\n",
      "Best Validation Accuracy: 0.9458\n",
      "Hyperparameter 55 : filter size: 256  kernel size: 10  units: 64  activation: relu\n",
      "Best Validation Loss: 0.1252\n",
      "Best Validation Accuracy: 0.9598\n",
      "Hyperparameter 56 : filter size: 256  kernel size: 10  units: 64  activation: tanh\n",
      "Best Validation Loss: 0.1570\n",
      "Best Validation Accuracy: 0.9466\n",
      "Hyperparameter 57 : filter size: 256  kernel size: 10  units: 128  activation: relu\n",
      "Best Validation Loss: 0.1326\n",
      "Best Validation Accuracy: 0.9580\n",
      "Hyperparameter 58 : filter size: 256  kernel size: 10  units: 128  activation: tanh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Validation Loss: 0.1491\n",
      "Best Validation Accuracy: 0.9484\n",
      "Hyperparameter 59 : filter size: 256  kernel size: 10  units: 256  activation: relu\n",
      "Best Validation Loss: 0.1293\n",
      "Best Validation Accuracy: 0.9584\n",
      "Hyperparameter 60 : filter size: 256  kernel size: 10  units: 256  activation: tanh\n",
      "Best Validation Loss: 0.1492\n",
      "Best Validation Accuracy: 0.9484\n"
     ]
    }
   ],
   "source": [
    "filter_sizes = [256] # \n",
    "kernel_sizes = [1,2,3,5,10]\n",
    "activations = ['relu','tanh']\n",
    "drop_out = 0.3\n",
    "units_list = [8,16,32,64,128,256]\n",
    "Best_acc2 = []\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "count = 0\n",
    "for filter_size in filter_sizes:\n",
    "    for kernel_size in kernel_sizes:\n",
    "        for units in units_list:\n",
    "            for activation in activations:\n",
    "                count+=1\n",
    "                print(\"Hyperparameter\", count, \": filter size:\", filter_size, \" kernel size:\", kernel_size, \" units:\", units, \" activation:\", activation)\n",
    "                model = keras.Sequential([\n",
    "                    layers.BatchNormalization(input_shape=(81, 1)),\n",
    "                    layers.Conv1D(filters=filter_size,kernel_size=kernel_size),\n",
    "                    layers.Dropout(drop_out),\n",
    "                    layers.BatchNormalization(),\n",
    "\n",
    "                    layers.Flatten(), # flatten out the layers\n",
    "                    layers.Dense(units=2*units,activation=activation),\n",
    "                    layers.Dropout(drop_out),\n",
    "                    layers.BatchNormalization(),\n",
    "\n",
    "                    layers.Dense(units=units,activation=activation),\n",
    "                    layers.BatchNormalization(),\n",
    "                    layers.Dense(1, activation='sigmoid'),\n",
    "                ])\n",
    "\n",
    "                opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "                model.compile(\n",
    "                    optimizer=opt,\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['binary_accuracy'],\n",
    "                )\n",
    "\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=128,\n",
    "                    epochs=50,\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                history_df = pd.DataFrame(history.history)\n",
    "#                 # Start the plot at epoch 0\n",
    "#                 history_df.loc[0:, ['loss', 'val_loss']].plot(title=\"Loss Plot with hyperparameter \"+str(count))\n",
    "#                 history_df.loc[0:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Acc Plot with hyperparameter \"+str(count))\n",
    "                Best_acc2.append(history_df['val_binary_accuracy'].max())\n",
    "    \n",
    "                print((\"Best Validation Loss: {:0.4f}\" +\\\n",
    "                      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n",
    "                      .format(history_df['val_loss'].min(), \n",
    "                              history_df['val_binary_accuracy'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9636920094490051"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(Best_acc2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
